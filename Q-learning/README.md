# Q-Learning avec Goal Dynamique

ImplÃ©mentation de Q-Learning pour un environnement GridWorld avec goal dynamique.

## ğŸ¯ CaractÃ©ristiques

### Environnement Dynamique
- **Goal mobile** : Change de position alÃ©atoirement Ã  chaque Ã©pisode
- **Agent (boule rouge)** : Se dÃ©place sur la grille
- **Features** : Position (row, col) + Distance Manhattan au goal

### Deux MÃ©thodes d'Apprentissage

## ğŸ“ Structure

```
Q-learning/
â”œâ”€â”€ episodic/                    # MÃ©thode Ã©pisodique
â”‚   â”œâ”€â”€ grid_env_dynamic.py     # Environnement
â”‚   â”œâ”€â”€ q_agent_episodic.py     # Agent Q-Learning Ã©pisodique
â”‚   â”œâ”€â”€ train_episodic.py       # Script d'entraÃ®nement
â”‚   â””â”€â”€ results_episodic/       # RÃ©sultats (crÃ©Ã© automatiquement)
â”‚
â”œâ”€â”€ iterative/                   # MÃ©thode itÃ©rative
â”‚   â”œâ”€â”€ grid_env_dynamic.py     # Environnement
â”‚   â”œâ”€â”€ q_agent_iterative.py    # Agent Q-Learning itÃ©ratif
â”‚   â”œâ”€â”€ train_iterative.py      # Script d'entraÃ®nement
â”‚   â””â”€â”€ results_iterative/      # RÃ©sultats (crÃ©Ã© automatiquement)
â”‚
â””â”€â”€ README.md
```

## ğŸ”„ MÃ©thode 1 : Ã‰pisodique

**Principe** : Collecte toutes les transitions d'un Ã©pisode, puis met Ã  jour la Q-table Ã  la fin.

### Avantages
- StabilitÃ© de l'apprentissage
- Peut utiliser des techniques de replay
- Bon pour des environnements dÃ©terministes

### Algorithme
```python
for episode in episodes:
    transitions = []
    while not done:
        action = choose_action(state)
        next_state, reward = env.step(action)
        transitions.append((state, action, reward, next_state))
    
    # Mise Ã  jour aprÃ¨s l'Ã©pisode
    for transition in transitions:
        update_q_table(transition)
```

### ExÃ©cution
```bash
cd episodic
python train_episodic.py
```

## âš¡ MÃ©thode 2 : ItÃ©rative

**Principe** : Met Ã  jour la Q-table immÃ©diatement aprÃ¨s chaque transition (step).

### Avantages
- Apprentissage plus rapide
- RÃ©agit immÃ©diatement aux nouvelles informations
- Standard pour Q-Learning

### Algorithme
```python
for episode in episodes:
    while not done:
        action = choose_action(state)
        next_state, reward = env.step(action)
        
        # Mise Ã  jour immÃ©diate
        update_q_table(state, action, reward, next_state)
```

### ExÃ©cution
```bash
cd iterative
python train_iterative.py
```

## ğŸ“Š Q-Learning Update Rule

Les deux mÃ©thodes utilisent la mÃªme rÃ¨gle de mise Ã  jour :

```
Q(s, a) â† Q(s, a) + Î± [r + Î³ max_a' Q(s', a') - Q(s, a)]
```

OÃ¹ :
- **Î±** (learning_rate) = 0.1 : Taux d'apprentissage
- **Î³** (gamma) = 0.99 : Facteur d'actualisation
- **r** : RÃ©compense
- **s'** : Ã‰tat suivant
- **max_a' Q(s', a')** : Meilleure valeur Q de l'Ã©tat suivant

## ğŸ® Features de l'Ã‰tat

Chaque Ã©tat est reprÃ©sentÃ© par :
1. **Position de l'agent** : (row, col)
2. **Distance au goal** : Distance de Manhattan

Exemple : `(2, 3, 4)` = Agent en (2,3), distance 4 du goal

## ğŸ† RÃ©compenses

- **Goal atteint** : +10.0
- **DÃ©placement normal** : -0.01
- **Obstacle** : -1.0
- **Mur** : -0.01

## ğŸ“ˆ RÃ©sultats

Les scripts sauvegardent automatiquement :
- **Courbes de progression** (PNG)
- **Statistiques dÃ©taillÃ©es** (JSON)
  - RÃ©compenses par Ã©pisode
  - Longueur des Ã©pisodes
  - Taux de succÃ¨s
  - Taille de la Q-table

## âš™ï¸ ParamÃ¨tres

### Environnement
```python
grid_size = 5                    # Grille 5x5
obstacles = [(2, 2)]            # Un obstacle
max_steps_per_episode = 100     # Limite de pas
```

### Agent
```python
learning_rate = 0.1             # Alpha
gamma = 0.99                    # Facteur d'actualisation
epsilon = 1.0                   # Exploration initiale
epsilon_decay = 0.995           # DÃ©croissance
epsilon_min = 0.01              # Exploration minimale
```

### EntraÃ®nement
```python
num_episodes = 500              # Nombre d'Ã©pisodes
render_frequency = 50           # Affichage tous les 50 Ã©pisodes
```

## ğŸ” Comparaison des MÃ©thodes

| Aspect | Ã‰pisodique | ItÃ©rative |
|--------|-----------|-----------|
| Mise Ã  jour | Fin d'Ã©pisode | Chaque step |
| Vitesse | Plus lent | Plus rapide |
| StabilitÃ© | Plus stable | Peut osciller |
| MÃ©moire | Buffer requis | Pas de buffer |
| Standard | Monte Carlo | Q-Learning classique |

## ğŸš€ Pour commencer

1. **MÃ©thode Ã©pisodique** :
```bash
cd episodic
python train_episodic.py
```

2. **MÃ©thode itÃ©rative** :
```bash
cd iterative
python train_iterative.py
```

Les deux afficheront :
- La grille en temps rÃ©el
- Les courbes de progression
- Les statistiques d'entraÃ®nement

## ğŸ“ Notes

- Le goal change Ã  chaque nouvel Ã©pisode
- L'agent apprend Ã  se diriger vers le goal quelle que soit sa position
- Les features incluent la distance, permettant une gÃ©nÃ©ralisation
- Epsilon-greedy pour Ã©quilibrer exploration/exploitation
