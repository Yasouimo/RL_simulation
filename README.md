# ğŸ® Reinforcement Learning - GridWorld

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.24+-orange.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.7+-green.svg)](https://matplotlib.org/)

> Comparaison de 4 algorithmes RL classiques sur un environnement GridWorld

---

<<<<<<< HEAD
=======
## ğŸ“Š RÃ©sultats

| MÃ©thode | SuccÃ¨s | RÃ©compense | Vitesse |
|---------|--------|------------|---------|
| ğŸ¥‡ **Value Iteration** | 95% | +9.2 | â­â­â­â­â­ |
| ğŸ¥ˆ **Q-Learning ItÃ©ratif** | 70% | +6.3 | â­â­â­â­ |
| ğŸ¥‰ **Q-Learning Ã‰pisodique** | 55% | +4.5 | â­â­â­ |
| ğŸ’€ **Random (Baseline)** | 5% | -3.2 | â­ |


---

>>>>>>> 47551b8e05cdb373e3d5644216becd86b57bde4f
## ğŸ¯ L'Environnement

```
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ ğŸ”´â”‚   â”‚   â”‚   â”‚   â”‚  ğŸ”´ Agent
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  ğŸŸ¡ Goal (dynamique)
â”‚   â”‚   â”‚ â¬›â”‚   â”‚   â”‚  â¬› Obstacle
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  
â”‚   â”‚   â”‚   â”‚   â”‚ ğŸŸ¡â”‚  RÃ©compenses:
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  +10 (goal)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  -0.01 (step)
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜  -1 (obstacle)
```

**Grille 5Ã—5** Â· Goal repositionnÃ© chaque Ã©pisode Â· Max 100 steps

---

## ğŸš€ Installation

```bash
pip install -r requirements.txt
```

---

## ğŸ§  1. Value Iteration (Planning)

**Logique** : Calcule mathÃ©matiquement la meilleure stratÃ©gie **avant** de jouer. Utilise l'Ã©quation de Bellman pour trouver la valeur optimale de chaque case, sachant toutes les rÃ¨gles du jeu.

```bash
cd "Value Iteration"
python main.py
```

![Value Iteration](Value%20Iteration/results/value_table_5x5_20251201_170501.png)

**Performance** : âœ… 95% succÃ¨s Â· +9.2 rÃ©compense Â· â­â­â­â­â­

---

## ğŸ¯ 2. Q-Learning ItÃ©ratif (Apprentissage Rapide)

**Logique** : Apprend en **jouant**. AprÃ¨s chaque action, met immÃ©diatement Ã  jour la Q-table (mÃ©moire des bonnes actions). Plus rÃ©actif car apprend step par step.

```bash
cd Q-learning/iterative
python train_iterative.py
```

![Iterative Training](Q-learning/iterative/results_iterative/training_iterative_20251201_175422.png)

**Performance** : âœ… 70% succÃ¨s Â· +6.3 rÃ©compense Â· â­â­â­â­

---

## ğŸ“¦ 3. Q-Learning Ã‰pisodique (Apprentissage par Ã‰pisode)

**Logique** : Apprend en **jouant** aussi, mais stocke toutes les actions d'un Ã©pisode en mÃ©moire et met Ã  jour la Q-table seulement **Ã  la fin**. Plus lent Ã  apprendre.

```bash
cd Q-learning/episodic
python train_episodic.py
```

![Episodic Training](Q-learning/episodic/results_episodic/training_episodic_20251201_175253.png)

**Performance** : âš ï¸ 55% succÃ¨s Â· +4.5 rÃ©compense Â· â­â­â­

---

## ğŸ² 4. Random Agent (Baseline)

**Logique** : **Aucune intelligence**. Choisit des actions complÃ¨tement au hasard, sans jamais apprendre. Sert Ã  prouver que l'apprentissage marche vraiment.

```bash
cd "Value Iteration Random"
python train_random.py
```

![Random Baseline](Value%20Iteration%20Random/results_random/random_baseline_20251201_182011.png)

**Performance** : âŒ 5% succÃ¨s Â· -3.2 rÃ©compense Â· â­

---

## ğŸ“Š Comparaison Finale

### Tableau RÃ©capitulatif

| MÃ©thode | SuccÃ¨s | RÃ©compense | Apprentissage |
|---------|--------|------------|---------------|
| ğŸ¥‡ **Value Iteration** | 95% | +9.2 | Hors-ligne (planning) |
| ğŸ¥ˆ **Q-Learning ItÃ©ratif** | 70% | +6.3 | En ligne (step) |
| ğŸ¥‰ **Q-Learning Ã‰pisodique** | 55% | +4.5 | En ligne (episode) |
| ğŸ’€ **Random** | 5% | -3.2 | Aucun |

### Analyse Graphique : Q-Learning ItÃ©ratif vs Ã‰pisodique

```bash
cd Q-learning
python compare_methods.py
```

![Comparison](Q-learning/comparison_results/comparison_20251201_173514.png)

<<<<<<< HEAD
**Conclusion** : ItÃ©ratif converge **2x plus rapide** et atteint **70% succÃ¨s** vs 55% pour Ã‰pisodique
=======
## ğŸ“ Structure

```
RL_exo/
â”œâ”€â”€ Value Iteration/      # Planning (goal statique)
â”œâ”€â”€ Q-learning/
â”‚   â”œâ”€â”€ episodic/         # Updates fin d'Ã©pisode
â”‚   â”œâ”€â”€ iterative/        # Updates chaque step
â”‚   â””â”€â”€ compare_methods.py
â””â”€â”€ Value Iteration Random/  # Baseline
```

---

## ğŸ“Š Visualisation 4-Panel



Chaque mÃ©thode affiche en temps rÃ©el :
- ğŸ—ºï¸ **GridWorld** : Agent, goal, obstacles
- ğŸ“ˆ **Courbes** : RÃ©compenses et longueurs
- ğŸ”¥ **Heatmap** : Q-values ou values
- ğŸ“‹ **Stats** : Taux de succÃ¨s, epsilon, etc.

---

## ğŸ” DiffÃ©rences ClÃ©s

### Value Iteration vs Q-Learning

| | Value Iteration | Q-Learning |
|---|---|---|
| **Type** | Planning | Learning |
| **Goal** | Statique | Dynamique âœ¨ |
| **Performance** | 95% | 70% |

### Q-Learning : ItÃ©ratif vs Ã‰pisodique

| | ItÃ©ratif | Ã‰pisodique |
|---|---|---|
| **Updates** | Chaque step | Fin d'Ã©pisode |
| **SuccÃ¨s** | 70% | 55% |
| **Vitesse** | âš¡ Rapide | ğŸ¢ Lent |


>>>>>>> 47551b8e05cdb373e3d5644216becd86b57bde4f

---

## ğŸ’¡ Ce Qu'On Apprend

### ğŸ¯ DiffÃ©rence ClÃ© : Planning vs Learning

- **Value Iteration** (planning) : RÃ©flÃ©chit **avant** de jouer â†’ connaÃ®t tout l'environnement â†’ **95% succÃ¨s**
- **Q-Learning** (learning) : Apprend **en jouant** â†’ dÃ©couvre l'environnement â†’ **55-70% succÃ¨s**

### âš¡ DiffÃ©rence ClÃ© : Updates ImmÃ©diates vs DiffÃ©rÃ©es

- **ItÃ©ratif** : Update aprÃ¨s chaque step â†’ apprentissage rapide â†’ **70% succÃ¨s**
- **Ã‰pisodique** : Update Ã  la fin d'Ã©pisode â†’ apprentissage lent â†’ **55% succÃ¨s**

### ğŸ“Š Pourquoi la Baseline Random ?

- **Random** : 5% succÃ¨s â†’ prouve que le problÃ¨me est **difficile**
- **Gain RL** : +65% de succÃ¨s â†’ prouve que **l'apprentissage fonctionne vraiment**

---

## ğŸ“ Structure du Projet

```
RL_exo/
â”œâ”€â”€ Value Iteration/           # Planning (goal statique)
â”œâ”€â”€ Q-learning/
â”‚   â”œâ”€â”€ episodic/              # Updates fin d'Ã©pisode
â”‚   â”œâ”€â”€ iterative/             # Updates chaque step
â”‚   â””â”€â”€ compare_methods.py     # Comparaison graphique
â””â”€â”€ Value Iteration Random/    # Baseline alÃ©atoire
```

---

## ğŸ“ Concepts ImplÃ©mentÃ©s

âœ… **Bellman Equation** Â· **Q-Learning** Â· **Epsilon-Greedy** Â· **State Augmentation** Â· **Q-Table Heatmaps** Â· **Real-time Visualization**

---

## ğŸ“š Documentation DÃ©taillÃ©e

- [`Value Iteration/README.md`](Value%20Iteration/README.md) - Planning classique
- [`Q-learning/README.md`](Q-learning/README.md) - Comparaison des mÃ©thodes
- [`Value Iteration Random/README.md`](Value%20Iteration%20Random/README.md) - Baseline

---

<div align="center">

**Technologies** : Python 3.8+ Â· NumPy Â· Matplotlib  
**Inspiration** : Sutton & Barto - "Reinforcement Learning: An Introduction"

**â­ Comparez les 4 mÃ©thodes pour voir la puissance de l'apprentissage ! â­**

</div>