import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from grid_env_dynamic import DynamicGridWorldEnv
from random_agent import RandomAgent
import json
import os
from datetime import datetime
import time


def display_info(ax, episode, stats, avg_reward, avg_length, 
                episode_rewards, episode_lengths):
    """
    Affiche les informations textuelles sur l'entraÃ®nement.
    """
    ax.clear()
    ax.axis('off')
    
    info_text = f"""
    ğŸ² AGENT ALÃ‰ATOIRE (BASELINE)
    
    Ã‰pisode: {episode}
    
    Performance (10 derniers):
      â€¢ RÃ©compense moyenne: {avg_reward:.2f}
      â€¢ Longueur moyenne: {avg_length:.1f} steps
    
    Agent:
      â€¢ Type: Random (pas d'apprentissage)
      â€¢ Actions prises: {stats['actions_taken']}
      â€¢ Exploration: 100% (toujours alÃ©atoire)
    
    Progression globale:
      â€¢ RÃ©compense max: {max(episode_rewards):.2f}
      â€¢ RÃ©compense min: {min(episode_rewards):.2f}
      â€¢ Moyenne totale: {np.mean(episode_rewards):.2f}
    
    SuccÃ¨s rÃ©cents (100 derniers):
    """
    
    if len(episode_rewards) >= 100:
        recent_success = sum(1 for r in episode_rewards[-100:] if r > 5)
        info_text += f"      â€¢ {recent_success}% d'Ã©pisodes rÃ©ussis"
    else:
        recent_success = sum(1 for r in episode_rewards if r > 5)
        total = len(episode_rewards)
        info_text += f"      â€¢ {recent_success}/{total} Ã©pisodes rÃ©ussis"
    
    ax.text(0.1, 0.5, info_text, transform=ax.transAxes,
           fontsize=11, verticalalignment='center',
           fontfamily='monospace',
           bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))


def train_random(num_episodes=500, grid_size=5, render_frequency=50):
    """
    Fait jouer un agent alÃ©atoire (baseline sans apprentissage).
    
    Args:
        num_episodes: Nombre d'Ã©pisodes
        grid_size: Taille de la grille
        render_frequency: FrÃ©quence d'affichage
    """
    print("="*60)
    print("AGENT ALÃ‰ATOIRE - BASELINE (PAS D'APPRENTISSAGE)")
    print("="*60)
    print(f"Nombre d'Ã©pisodes: {num_episodes}")
    print(f"Taille de la grille: {grid_size}x{grid_size}")
    print()
    print("âš ï¸  Cet agent choisit des actions ALÃ‰ATOIRES")
    print("âš ï¸  Il ne fait AUCUN apprentissage")
    print("âš ï¸  Il sert de BASELINE pour comparer avec les mÃ©thodes intelligentes")
    print()
    
    # CrÃ©er l'environnement
    env = DynamicGridWorldEnv(
        grid_size=grid_size,
        obstacles=[(2, 2)],
        step_cost=-0.01,
        goal_reward=10.0,
        max_steps_per_episode=100
    )
    
    # CrÃ©er l'agent alÃ©atoire
    agent = RandomAgent(num_actions=4)
    
    # Statistiques
    episode_rewards = []
    episode_lengths = []
    success_rate = []
    
    # Configuration de la visualisation
    plt.ion()
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    ax_env = axes[0, 0]
    ax_stats = axes[0, 1]
    ax_comparison = axes[1, 0]
    ax_info = axes[1, 1]
    
    print("DÃ©but de l'exÃ©cution...")
    print()
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_length = 0
        done = False
        
        # ExÃ©cuter l'Ã©pisode avec actions alÃ©atoires
        while not done:
            # Action COMPLÃˆTEMENT ALÃ‰ATOIRE
            action = agent.get_action(state)
            
            # Effectuer l'action
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            state = next_state
        
        # Enregistrer les statistiques
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # Calculer le taux de succÃ¨s
        if len(episode_rewards) >= 100:
            recent_rewards = episode_rewards[-100:]
            success = sum(1 for r in recent_rewards if r > 5) / 100
            success_rate.append(success)
        
        # Affichage pÃ©riodique
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            stats = agent.get_stats()
            print(f"Ã‰pisode {episode + 1}/{num_episodes}")
            print(f"  RÃ©compense moyenne (10 derniers): {avg_reward:.2f}")
            print(f"  Longueur moyenne (10 derniers): {avg_length:.1f}")
            print(f"  Actions alÃ©atoires prises: {stats['actions_taken']}")
            print()
        
        # Visualisation pÃ©riodique
        if (episode + 1) % render_frequency == 0:
            # Afficher l'environnement
            env.render(fig=fig, ax=ax_env)
            
            # Afficher les informations
            stats = agent.get_stats()
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            display_info(ax_info, episode + 1, stats, avg_reward, avg_length,
                        episode_rewards, episode_lengths)
            
            # Courbe de performance
            ax_stats.clear()
            ax_stats.plot(episode_rewards, alpha=0.3, color='red', label='RÃ©compense')
            if len(episode_rewards) >= 10:
                moving_avg = np.convolve(episode_rewards, 
                                        np.ones(10)/10, mode='valid')
                ax_stats.plot(range(9, len(episode_rewards)), moving_avg, 
                            color='red', linewidth=2, label='Moyenne mobile (10)')
            ax_stats.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
            ax_stats.set_xlabel('Ã‰pisode')
            ax_stats.set_ylabel('RÃ©compense')
            ax_stats.set_title('Performance de l\'Agent AlÃ©atoire')
            ax_stats.legend()
            ax_stats.grid(True, alpha=0.3)
            
            # Comparaison avec une baseline thÃ©orique
            ax_comparison.clear()
            ax_comparison.hist(episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards,
                             bins=20, alpha=0.7, color='red', edgecolor='black')
            ax_comparison.axvline(x=0, color='gray', linestyle='--', linewidth=2, label='Seuil neutre')
            ax_comparison.axvline(x=5, color='green', linestyle='--', linewidth=2, label='Seuil succÃ¨s')
            ax_comparison.set_xlabel('RÃ©compense')
            ax_comparison.set_ylabel('FrÃ©quence')
            ax_comparison.set_title('Distribution des RÃ©compenses')
            ax_comparison.legend()
            ax_comparison.grid(True, alpha=0.3, axis='y')
            
            plt.tight_layout()
            plt.pause(0.5)
            time.sleep(0.3)
    
    print("="*60)
    print("EXÃ‰CUTION TERMINÃ‰E")
    print("="*60)
    
    # Statistiques finales
    print(f"\nStatistiques finales de l'Agent AlÃ©atoire:")
    print(f"  RÃ©compense moyenne (100 derniers): {np.mean(episode_rewards[-100:]):.2f}")
    print(f"  Longueur moyenne (100 derniers): {np.mean(episode_lengths[-100:]):.1f}")
    if success_rate:
        print(f"  Taux de succÃ¨s (100 derniers): {success_rate[-1]*100:.1f}%")
    else:
        recent_success = sum(1 for r in episode_rewards if r > 5)
        print(f"  Taux de succÃ¨s: {recent_success/len(episode_rewards)*100:.1f}%")
    print(f"  Actions alÃ©atoires totales: {agent.get_stats()['actions_taken']}")
    
    # Sauvegarder les rÃ©sultats
    output_folder = "results_random"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Sauvegarder les courbes
    plt.savefig(os.path.join(output_folder, f"random_baseline_{timestamp}.png"), 
                dpi=300, bbox_inches='tight')
    print(f"\nâœ“ Courbes sauvegardÃ©es dans {output_folder}/")
    
    # Sauvegarder les statistiques
    stats_data = {
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'success_rate': success_rate,
        'final_stats': agent.get_stats(),
        'config': {
            'num_episodes': num_episodes,
            'grid_size': grid_size,
            'agent_type': 'Random'
        }
    }
    
    with open(os.path.join(output_folder, f"stats_random_{timestamp}.json"), 'w') as f:
        json.dump(stats_data, f, indent=2)
    
    print(f"âœ“ Statistiques sauvegardÃ©es dans {output_folder}/")
    
    print()
    print("="*60)
    print("ğŸ“Š COMPARAISON ATTENDUE AVEC LES MÃ‰THODES INTELLIGENTES:")
    print("="*60)
    print("L'agent alÃ©atoire devrait avoir:")
    print("  âŒ TrÃ¨s faible taux de succÃ¨s (~0-10%)")
    print("  âŒ RÃ©compenses majoritairement nÃ©gatives")
    print("  âŒ Pas d'amÃ©lioration au fil du temps")
    print("  âŒ Longueur maximale des Ã©pisodes (timeout)")
    print()
    print("ComparÃ© Ã  Q-Learning qui devrait avoir:")
    print("  âœ“ Taux de succÃ¨s ~50-70%")
    print("  âœ“ RÃ©compenses positives")
    print("  âœ“ AmÃ©lioration visible")
    print("  âœ“ Ã‰pisodes courts et efficaces")
    print("="*60)
    
    plt.ioff()
    plt.show()
    
    return agent, env


if __name__ == "__main__":
    agent, env = train_random(num_episodes=500, grid_size=5, render_frequency=50)
