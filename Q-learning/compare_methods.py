import json
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime


class MethodComparator:
    """
    Compare les performances des m√©thodes √©pisodique et it√©rative.
    """
    
    def __init__(self, episodic_stats_file, iterative_stats_file):
        """
        Initialise le comparateur avec les fichiers de statistiques.
        
        Args:
            episodic_stats_file: Chemin vers les stats √©pisodiques
            iterative_stats_file: Chemin vers les stats it√©ratives
        """
        with open(episodic_stats_file, 'r') as f:
            self.episodic_data = json.load(f)
        
        with open(iterative_stats_file, 'r') as f:
            self.iterative_data = json.load(f)
    
    def calculate_metrics(self):
        """
        Calcule des m√©triques de comparaison d√©taill√©es.
        
        Returns:
            dict: M√©triques pour les deux m√©thodes
        """
        metrics = {
            'episodic': self._compute_method_metrics(self.episodic_data),
            'iterative': self._compute_method_metrics(self.iterative_data)
        }
        
        return metrics
    
    def _compute_method_metrics(self, data):
        """
        Calcule les m√©triques pour une m√©thode donn√©e.
        """
        rewards = np.array(data['episode_rewards'])
        lengths = np.array(data['episode_lengths'])
        
        # D√©finir le seuil de succ√®s (r√©compense > 5 signifie qu'on a atteint le goal)
        success_threshold = 5
        successes = rewards > success_threshold
        
        # M√©triques globales
        metrics = {
            # Performance g√©n√©rale
            'mean_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'median_reward': np.median(rewards),
            'max_reward': np.max(rewards),
            'min_reward': np.min(rewards),
            
            # Performance finale (100 derniers √©pisodes)
            'final_mean_reward': np.mean(rewards[-100:]),
            'final_std_reward': np.std(rewards[-100:]),
            'final_median_reward': np.median(rewards[-100:]),
            
            # Longueur des √©pisodes
            'mean_length': np.mean(lengths),
            'final_mean_length': np.mean(lengths[-100:]),
            
            # Taux de succ√®s
            'overall_success_rate': np.mean(successes) * 100,
            'final_success_rate': np.mean(successes[-100:]) * 100,
            
            # Convergence
            'episodes_to_50_percent_success': self._episodes_to_threshold(successes, 0.5),
            'episodes_to_70_percent_success': self._episodes_to_threshold(successes, 0.7),
            
            # Stabilit√© (√©cart-type sur fen√™tres glissantes)
            'early_stability': np.std(rewards[:100]) if len(rewards) >= 100 else np.std(rewards),
            'late_stability': np.std(rewards[-100:]),
            
            # Efficacit√© d'apprentissage
            'learning_speed': self._compute_learning_speed(rewards),
            'sample_efficiency': self._compute_sample_efficiency(rewards, successes),
            
            # Q-table
            'q_table_size': data['final_stats']['q_table_size'],
            'final_epsilon': data['final_stats']['epsilon'],
        }
        
        # Ajouter le nombre de mises √† jour si disponible (it√©ratif seulement)
        if 'update_count' in data['final_stats']:
            metrics['update_count'] = data['final_stats']['update_count']
        
        return metrics
    
    def _episodes_to_threshold(self, successes, threshold):
        """
        Calcule le nombre d'√©pisodes n√©cessaires pour atteindre un seuil de succ√®s.
        """
        window_size = 100
        for i in range(window_size, len(successes)):
            if np.mean(successes[i-window_size:i]) >= threshold:
                return i
        return len(successes)  # Pas atteint
    
    def _compute_learning_speed(self, rewards):
        """
        Calcule la vitesse d'apprentissage (pente de la courbe de r√©compense).
        """
        if len(rewards) < 50:
            return 0
        
        # Calculer la pente sur les 200 premiers √©pisodes
        x = np.arange(min(200, len(rewards)))
        y = rewards[:len(x)]
        
        # R√©gression lin√©aire simple
        slope = np.polyfit(x, y, 1)[0]
        return slope
    
    def _compute_sample_efficiency(self, rewards, successes):
        """
        Efficacit√© d'√©chantillonnage : r√©compense moyenne / √©pisodes n√©cessaires.
        """
        episodes_to_converge = self._episodes_to_threshold(successes, 0.5)
        if episodes_to_converge == 0:
            return 0
        return np.mean(rewards[:episodes_to_converge]) / episodes_to_converge
    
    def print_comparison(self, metrics):
        """
        Affiche une comparaison d√©taill√©e des m√©triques.
        """
        print("="*80)
        print("COMPARAISON DES M√âTHODES Q-LEARNING")
        print("="*80)
        print()
        
        # Tableau de comparaison
        print(f"{'M√âTRIQUE':<45} {'√âPISODIQUE':<15} {'IT√âRATIVE':<15} {'GAGNANT':<10}")
        print("-" * 80)
        
        comparisons = [
            ("Performance Globale", "", "", ""),
            ("  R√©compense moyenne", 'mean_reward', 'mean_reward', 'higher'),
            ("  R√©compense m√©diane", 'median_reward', 'median_reward', 'higher'),
            ("  R√©compense max", 'max_reward', 'max_reward', 'higher'),
            ("", "", "", ""),
            
            ("Performance Finale (100 derniers)", "", "", ""),
            ("  R√©compense moyenne", 'final_mean_reward', 'final_mean_reward', 'higher'),
            ("  √âcart-type", 'final_std_reward', 'final_std_reward', 'lower'),
            ("  Longueur moyenne", 'final_mean_length', 'final_mean_length', 'lower'),
            ("", "", "", ""),
            
            ("Taux de Succ√®s (%)", "", "", ""),
            ("  Global", 'overall_success_rate', 'overall_success_rate', 'higher'),
            ("  Final (100 derniers)", 'final_success_rate', 'final_success_rate', 'higher'),
            ("", "", "", ""),
            
            ("Convergence", "", "", ""),
            ("  √âpisodes pour 50% succ√®s", 'episodes_to_50_percent_success', 
             'episodes_to_50_percent_success', 'lower'),
            ("  √âpisodes pour 70% succ√®s", 'episodes_to_70_percent_success', 
             'episodes_to_70_percent_success', 'lower'),
            ("", "", "", ""),
            
            ("Stabilit√©", "", "", ""),
            ("  D√©but (std premiers 100)", 'early_stability', 'early_stability', 'lower'),
            ("  Fin (std derniers 100)", 'late_stability', 'late_stability', 'lower'),
            ("", "", "", ""),
            
            ("Efficacit√©", "", "", ""),
            ("  Vitesse d'apprentissage", 'learning_speed', 'learning_speed', 'higher'),
            ("  Efficacit√© d'√©chantillonnage", 'sample_efficiency', 
             'sample_efficiency', 'higher'),
            ("", "", "", ""),
            
            ("Autres", "", "", ""),
            ("  Taille Q-table", 'q_table_size', 'q_table_size', 'equal'),
            ("  Epsilon final", 'final_epsilon', 'final_epsilon', 'equal'),
        ]
        
        score_episodic = 0
        score_iterative = 0
        
        for item in comparisons:
            if len(item[1]) == 0:  # Ligne de titre ou vide
                print(f"{item[0]:<45}")
                continue
            
            metric_name = item[0]
            episodic_key = item[1]
            iterative_key = item[2]
            comparison_type = item[3]
            
            episodic_val = metrics['episodic'].get(episodic_key, 0)
            iterative_val = metrics['iterative'].get(iterative_key, 0)
            
            # D√©terminer le gagnant
            winner = ""
            if comparison_type == 'higher':
                if episodic_val > iterative_val:
                    winner = "üìó √âpisodique"
                    score_episodic += 1
                elif iterative_val > episodic_val:
                    winner = "üìò It√©rative"
                    score_iterative += 1
                else:
                    winner = "‚öñÔ∏è √âgalit√©"
            elif comparison_type == 'lower':
                if episodic_val < iterative_val:
                    winner = "üìó √âpisodique"
                    score_episodic += 1
                elif iterative_val < episodic_val:
                    winner = "üìò It√©rative"
                    score_iterative += 1
                else:
                    winner = "‚öñÔ∏è √âgalit√©"
            else:  # equal
                winner = "‚öñÔ∏è √âgalit√©"
            
            print(f"{metric_name:<45} {episodic_val:<15.3f} {iterative_val:<15.3f} {winner:<10}")
        
        print("-" * 80)
        print()
        
        # R√©sum√©
        print("="*80)
        print("R√âSUM√â")
        print("="*80)
        print(f"Score √âpisodique: {score_episodic} points")
        print(f"Score It√©rative: {score_iterative} points")
        print()
        
        if score_iterative > score_episodic:
            print("üèÜ GAGNANT: M√âTHODE IT√âRATIVE")
            advantage = score_iterative - score_episodic
            print(f"   Avantage de {advantage} points")
        elif score_episodic > score_iterative:
            print("üèÜ GAGNANT: M√âTHODE √âPISODIQUE")
            advantage = score_episodic - score_iterative
            print(f"   Avantage de {advantage} points")
        else:
            print("‚öñÔ∏è √âGALIT√â PARFAITE")
        
        print()
        
        # Analyse d√©taill√©e
        print("="*80)
        print("ANALYSE D√âTAILL√âE")
        print("="*80)
        print()
        
        print("üìä Points Forts de la M√©thode IT√âRATIVE:")
        if metrics['iterative']['final_success_rate'] > metrics['episodic']['final_success_rate']:
            diff = metrics['iterative']['final_success_rate'] - metrics['episodic']['final_success_rate']
            print(f"  ‚úì Meilleur taux de succ√®s final: +{diff:.1f}%")
        
        if metrics['iterative']['final_mean_reward'] > metrics['episodic']['final_mean_reward']:
            diff = metrics['iterative']['final_mean_reward'] - metrics['episodic']['final_mean_reward']
            print(f"  ‚úì Meilleure r√©compense finale: +{diff:.2f}")
        
        if metrics['iterative']['learning_speed'] > metrics['episodic']['learning_speed']:
            print(f"  ‚úì Apprentissage plus rapide")
        
        if 'update_count' in metrics['iterative']:
            print(f"  ‚úì Nombre de mises √† jour: {metrics['iterative']['update_count']}")
        
        print()
        print("üìä Points Forts de la M√©thode √âPISODIQUE:")
        if metrics['episodic']['late_stability'] < metrics['iterative']['late_stability']:
            print(f"  ‚úì Plus stable en fin d'entra√Ænement")
        
        if metrics['episodic']['final_success_rate'] > metrics['iterative']['final_success_rate']:
            diff = metrics['episodic']['final_success_rate'] - metrics['iterative']['final_success_rate']
            print(f"  ‚úì Meilleur taux de succ√®s final: +{diff:.1f}%")
        
        print()
        
        # Recommandation
        print("="*80)
        print("RECOMMANDATION")
        print("="*80)
        print()
        
        if score_iterative > score_episodic + 2:
            print("üí° Pour ce probl√®me, la m√©thode IT√âRATIVE est recommand√©e:")
            print("   - Apprentissage plus efficace")
            print("   - Meilleures performances finales")
            print("   - Convergence plus rapide")
        elif score_episodic > score_iterative + 2:
            print("üí° Pour ce probl√®me, la m√©thode √âPISODIQUE est recommand√©e:")
            print("   - Plus stable")
            print("   - Meilleure g√©n√©ralisation")
        else:
            print("üí° Les deux m√©thodes sont √©quivalentes pour ce probl√®me.")
            print("   Choisir selon les pr√©f√©rences:")
            print("   - It√©rative: plus standard et rapide")
            print("   - √âpisodique: plus stable et th√©orique")
        
        print()
    
    def plot_comparison(self, save_path='comparison_plots.png'):
        """
        Cr√©e des graphiques de comparaison.
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Comparaison des M√©thodes Q-Learning', fontsize=16, fontweight='bold')
        
        episodic_rewards = np.array(self.episodic_data['episode_rewards'])
        iterative_rewards = np.array(self.iterative_data['episode_rewards'])
        
        episodic_lengths = np.array(self.episodic_data['episode_lengths'])
        iterative_lengths = np.array(self.iterative_data['episode_lengths'])
        
        # 1. R√©compenses brutes
        ax = axes[0, 0]
        ax.plot(episodic_rewards, alpha=0.3, color='blue', label='√âpisodique')
        ax.plot(iterative_rewards, alpha=0.3, color='green', label='It√©rative')
        ax.set_xlabel('√âpisode')
        ax.set_ylabel('R√©compense')
        ax.set_title('R√©compenses par √âpisode')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Moyennes mobiles
        ax = axes[0, 1]
        window = 50
        if len(episodic_rewards) >= window:
            ep_smooth = np.convolve(episodic_rewards, np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(episodic_rewards)), ep_smooth, 
                   color='blue', linewidth=2, label='√âpisodique (MA-50)')
        
        if len(iterative_rewards) >= window:
            it_smooth = np.convolve(iterative_rewards, np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(iterative_rewards)), it_smooth, 
                   color='green', linewidth=2, label='It√©rative (MA-50)')
        
        ax.set_xlabel('√âpisode')
        ax.set_ylabel('R√©compense (Moyenne Mobile)')
        ax.set_title('R√©compenses Liss√©es')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. Taux de succ√®s cumulatif
        ax = axes[0, 2]
        window = 100
        ep_success_rate = []
        it_success_rate = []
        
        for i in range(window, len(episodic_rewards)):
            ep_success_rate.append(np.mean(episodic_rewards[i-window:i] > 5) * 100)
        
        for i in range(window, len(iterative_rewards)):
            it_success_rate.append(np.mean(iterative_rewards[i-window:i] > 5) * 100)
        
        ax.plot(range(window, len(episodic_rewards)), ep_success_rate, 
               color='blue', linewidth=2, label='√âpisodique')
        ax.plot(range(window, len(iterative_rewards)), it_success_rate, 
               color='green', linewidth=2, label='It√©rative')
        ax.set_xlabel('√âpisode')
        ax.set_ylabel('Taux de Succ√®s (%)')
        ax.set_title(f'Taux de Succ√®s (Fen√™tre {window})')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 100])
        
        # 4. Longueur des √©pisodes
        ax = axes[1, 0]
        ax.plot(episodic_lengths, alpha=0.3, color='blue', label='√âpisodique')
        ax.plot(iterative_lengths, alpha=0.3, color='green', label='It√©rative')
        ax.set_xlabel('√âpisode')
        ax.set_ylabel('Longueur (steps)')
        ax.set_title('Longueur des √âpisodes')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 5. Distribution des r√©compenses finales
        ax = axes[1, 1]
        ax.hist(episodic_rewards[-100:], bins=20, alpha=0.5, color='blue', 
               label='√âpisodique', edgecolor='black')
        ax.hist(iterative_rewards[-100:], bins=20, alpha=0.5, color='green', 
               label='It√©rative', edgecolor='black')
        ax.set_xlabel('R√©compense')
        ax.set_ylabel('Fr√©quence')
        ax.set_title('Distribution (100 derniers √©pisodes)')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        # 6. Box plot comparatif
        ax = axes[1, 2]
        data_to_plot = [episodic_rewards[-100:], iterative_rewards[-100:]]
        bp = ax.boxplot(data_to_plot, labels=['√âpisodique', 'It√©rative'],
                       patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        bp['boxes'][1].set_facecolor('lightgreen')
        ax.set_ylabel('R√©compense')
        ax.set_title('Comparaison (100 derniers √©pisodes)')
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n‚úì Graphiques sauvegard√©s: {save_path}")
        
        return fig


def compare_latest_results():
    """
    Compare les r√©sultats les plus r√©cents des deux m√©thodes.
    """
    # Trouver les fichiers les plus r√©cents
    episodic_folder = "episodic/results_episodic"
    iterative_folder = "iterative/results_iterative"
    
    episodic_files = [f for f in os.listdir(episodic_folder) if f.endswith('.json')]
    iterative_files = [f for f in os.listdir(iterative_folder) if f.endswith('.json')]
    
    if not episodic_files or not iterative_files:
        print("Erreur: Fichiers de statistiques non trouv√©s!")
        return
    
    episodic_latest = sorted(episodic_files)[-1]
    iterative_latest = sorted(iterative_files)[-1]
    
    episodic_path = os.path.join(episodic_folder, episodic_latest)
    iterative_path = os.path.join(iterative_folder, iterative_latest)
    
    print(f"Comparaison des fichiers:")
    print(f"  √âpisodique: {episodic_latest}")
    print(f"  It√©rative: {iterative_latest}")
    print()
    
    # Cr√©er le comparateur
    comparator = MethodComparator(episodic_path, iterative_path)
    
    # Calculer les m√©triques
    metrics = comparator.calculate_metrics()
    
    # Afficher la comparaison
    comparator.print_comparison(metrics)
    
    # Cr√©er les graphiques
    output_folder = "comparison_results"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(output_folder, f"comparison_{timestamp}.png")
    
    comparator.plot_comparison(save_path=plot_path)
    
    plt.show()


if __name__ == "__main__":
    compare_latest_results()
