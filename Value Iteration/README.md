# GridWorld - Reinforcement Learning

Projet d'Apprentissage par Renforcement avec environnement GridWorld.

## ğŸ“‹ Structure du Projet

```
RL_exo/
â”œâ”€â”€ grid_env.py          # Environnement GridWorld
â”œâ”€â”€ agents.py            # Agents (Random et Value Iteration)
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ config.json          # Fichier de configuration
â”œâ”€â”€ requirements.txt     # DÃ©pendances Python
â””â”€â”€ results/             # Dossier des rÃ©sultats (crÃ©Ã© automatiquement)
```

## ğŸš€ Installation

```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### MÃ©thode 1 : Fichier de configuration (RecommandÃ©e)

Ã‰ditez le fichier `config.json` :

```json
{
  "grid_size": 5,
  "start_pos": [0, 0],
  "goal_pos": [4, 4],
  "obstacles": [
    [2, 2],
    [3, 2]
  ],
  "step_cost": -0.01,
  "animation_speed": 3,
  "save_figures": true,
  "output_folder": "results"
}
```

**ParamÃ¨tres :**
- `grid_size` : Taille de la grille (ex: 5 pour 5x5)
- `start_pos` : Position de dÃ©part [ligne, colonne]
- `goal_pos` : Position du but [ligne, colonne]
- `obstacles` : Liste des obstacles [[ligne1, col1], [ligne2, col2], ...]
- `step_cost` : PÃ©nalitÃ© par dÃ©placement (ex: -0.01)
- `animation_speed` : Vitesse (1=trÃ¨s lent, 2=lent, 3=normal, 4=rapide)
- `save_figures` : Sauvegarder les figures (true/false)
- `output_folder` : Dossier de sauvegarde des rÃ©sultats

### MÃ©thode 2 : Mode interactif

Le programme vous demandera tous les paramÃ¨tres au dÃ©marrage.

## â–¶ï¸ ExÃ©cution

```bash
python main.py
```

Au lancement, choisissez :
- **Option 1** : Charger la configuration depuis `config.json`
- **Option 2** : Mode interactif (saisie manuelle)

## ğŸ“Š RÃ©sultats

Les figures et valeurs sont sauvegardÃ©es dans le dossier `results/` :
- **PNG** : Visualisation de la grille avec heatmap des valeurs
- **TXT** : Matrice des valeurs d'Ã©tats

Nom des fichiers : `value_table_5x5_YYYYMMDD_HHMMSS.png`

## ğŸ® FonctionnalitÃ©s

1. **Agent AlÃ©atoire** : DÃ©placement alÃ©atoire (10 Ã©tapes)
2. **Value Iteration** : Apprentissage de la politique optimale
3. **Visualisation** : Heatmap des valeurs d'Ã©tats
4. **Agent EntraÃ®nÃ©** : DÃ©monstration du chemin optimal

## ğŸ“ Exemple de Configuration

### Grille simple (5x5)
```json
{
  "grid_size": 5,
  "start_pos": [0, 0],
  "goal_pos": [4, 4],
  "obstacles": [[2, 2], [3, 2]],
  "animation_speed": 3,
  "save_figures": true
}
```

### Grille complexe (10x10)
```json
{
  "grid_size": 10,
  "start_pos": [0, 0],
  "goal_pos": [9, 9],
  "obstacles": [
    [3, 3], [3, 4], [3, 5],
    [6, 2], [6, 3], [6, 4],
    [7, 7], [8, 7]
  ],
  "animation_speed": 4,
  "save_figures": true
}
```

## ğŸ”§ DÃ©pendances

- `numpy` : Calculs matriciels
- `matplotlib` : Visualisation

## ğŸ“š Algorithme

**Value Iteration** utilise l'Ã©quation de Bellman :

```
V(s) = max_a [R(s,a) + Î³ * V(s')]
```

- **V(s)** : Valeur de l'Ã©tat s
- **R(s,a)** : RÃ©compense immÃ©diate
- **Î³** : Facteur d'actualisation (gamma = 0.9)
- **s'** : Ã‰tat suivant

## ğŸ¯ Objectif

L'agent apprend Ã  atteindre le but (case dorÃ©e 'G') en Ã©vitant les obstacles (cases grises 'X') tout en minimisant le nombre de dÃ©placements.
