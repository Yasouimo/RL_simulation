# ğŸ® Projet Reinforcement Learning - GridWorld

> ImplÃ©mentations comparatives d'algorithmes de Reinforcement Learning classiques (sans Deep RL)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.24+-orange.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.7+-green.svg)](https://matplotlib.org/)

---

## ğŸ“‹ Vue d'Ensemble

Ce projet compare **4 approches** pour rÃ©soudre un problÃ¨me de navigation dans une grille (GridWorld) :

| MÃ©thode | Type | Apprentissage | Taux de SuccÃ¨s |
|---------|------|---------------|----------------|
| ğŸ§  **Value Iteration** | Planning | Hors-ligne | ~95% |
| ğŸ¯ **Q-Learning ItÃ©ratif** | RL | En ligne (step) | ~70% |
| ğŸ“¦ **Q-Learning Ã‰pisodique** | RL | En ligne (episode) | ~55% |
| ğŸ² **Random Agent** | Baseline | Aucun | ~5% |

---

## ğŸ¯ Environnement GridWorld

```
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ ğŸ”´â”‚   â”‚   â”‚   â”‚   â”‚  ğŸ”´ Agent (position alÃ©atoire)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  ğŸŸ¡ Goal (dynamique)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â¬› Obstacle
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  
â”‚   â”‚   â”‚ â¬›â”‚   â”‚   â”‚  RÃ©compenses:
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â€¢ Goal: +10.0
â”‚   â”‚   â”‚   â”‚   â”‚ ğŸŸ¡â”‚    â€¢ Step: -0.01
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â€¢ Obstacle: -1.0
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

**CaractÃ©ristiques** :
- Grille 5Ã—5 configurable
- Goal repositionnÃ© alÃ©atoirement chaque Ã©pisode
- Ã‰tat enrichi : `(position, distance_manhattan)`
- Maximum 100 steps par Ã©pisode

---

## ğŸ“‚ Structure du Projet

```
RL_exo/
â”‚
â”œâ”€â”€ ğŸ“ Value Iteration/              # Algorithme de planning classique
â”‚   â”œâ”€â”€ grid_env.py                  # Environnement (goal statique)
â”‚   â”œâ”€â”€ agents.py                    # Value Iteration + Random
â”‚   â”œâ”€â”€ main.py                      # Script principal
â”‚   â”œâ”€â”€ config.json                  # Configuration
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“ Q-learning/                   # Algorithmes d'apprentissage
â”‚   â”œâ”€â”€ ğŸ“ episodic/                 # Updates Ã  la fin de l'Ã©pisode
â”‚   â”‚   â”œâ”€â”€ grid_env_dynamic.py
â”‚   â”‚   â”œâ”€â”€ q_agent_episodic.py
â”‚   â”‚   â”œâ”€â”€ train_episodic.py
â”‚   â”‚   â””â”€â”€ results_episodic/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ iterative/                # Updates aprÃ¨s chaque step
â”‚   â”‚   â”œâ”€â”€ grid_env_dynamic.py
â”‚   â”‚   â”œâ”€â”€ q_agent_iterative.py
â”‚   â”‚   â”œâ”€â”€ train_iterative.py
â”‚   â”‚   â””â”€â”€ results_iterative/
â”‚   â”‚
â”‚   â”œâ”€â”€ compare_methods.py           # Outil de comparaison
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“ Value Iteration Random/       # Baseline (actions alÃ©atoires)
â”‚   â”œâ”€â”€ grid_env_dynamic.py
â”‚   â”œâ”€â”€ random_agent.py
â”‚   â”œâ”€â”€ train_random.py
â”‚   â””â”€â”€ results_random/
â”‚
â”œâ”€â”€ requirements.txt                 # DÃ©pendances Python
â””â”€â”€ README.md                        # Ce fichier
```

---

## ğŸš€ Installation & ExÃ©cution

### 1ï¸âƒ£ Installation

```bash
pip install -r requirements.txt
```

**DÃ©pendances** : `numpy`, `matplotlib` uniquement (pas de Deep RL)

### 2ï¸âƒ£ ExÃ©cution des DiffÃ©rentes MÃ©thodes

#### ğŸ§  Value Iteration (Planning)

```bash
cd "Value Iteration"
python main.py
```

**RÃ©sultat attendu** : ~95% de succÃ¨s, convergence rapide

#### ğŸ¯ Q-Learning ItÃ©ratif (Meilleure performance)

```bash
cd Q-learning/iterative
python train_iterative.py
```

**RÃ©sultat attendu** : ~70% de succÃ¨s, apprentissage stable

#### ğŸ“¦ Q-Learning Ã‰pisodique

```bash
cd Q-learning/episodic
python train_episodic.py
```

**RÃ©sultat attendu** : ~55% de succÃ¨s, moins stable

#### ğŸ² Random Agent (Baseline)

```bash
cd "Value Iteration Random"
python train_random.py
```

**RÃ©sultat attendu** : ~5% de succÃ¨s (dÃ©montre l'utilitÃ© de l'apprentissage)

### 3ï¸âƒ£ Comparaison des MÃ©thodes Q-Learning

```bash
cd Q-learning
python compare_methods.py
```

GÃ©nÃ¨re **6 graphiques comparatifs** avec **15+ mÃ©triques**.

---

## ğŸ“Š Visualisations

Chaque mÃ©thode gÃ©nÃ¨re une **interface 4-panel** en temps rÃ©el :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ—ºï¸ GridWorld   â”‚  ğŸ“ˆ Performance â”‚
â”‚  (environnement)â”‚  (courbes)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”¥ Q-Table     â”‚  ğŸ“‹ Statistiquesâ”‚
â”‚  (heatmap)      â”‚  (mÃ©triques)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Contenu** :
- **Panel 1** : Grille avec agent, goal, obstacles, values/Q-values
- **Panel 2** : Courbes de rÃ©compense et longueur d'Ã©pisode
- **Panel 3** : Heatmap des Q-values (Q-Learning) ou values (VI)
- **Panel 4** : Statistiques textuelles (taux de succÃ¨s, epsilon, etc.)

---

## ğŸ† RÃ©sultats Comparatifs

### Performance Finale (500 Ã©pisodes)

| MÃ©thode | SuccÃ¨s | RÃ©compense Moy. | Longueur Moy. | StabilitÃ© |
|---------|--------|-----------------|---------------|-----------|
| ğŸ¥‡ **Value Iteration** | 95% | +9.2 | 15 steps | â­â­â­â­â­ |
| ğŸ¥ˆ **Q-Learning ItÃ©ratif** | 70% | +6.3 | 45 steps | â­â­â­â­ |
| ğŸ¥‰ **Q-Learning Ã‰pisodique** | 55% | +4.5 | 55 steps | â­â­â­ |
| ğŸ’€ **Random Agent** | 5% | -3.2 | 100 steps | â­ |

### Comparaison Visuelle

```
Performance (Taux de SuccÃ¨s)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Value Iteration (95%)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Q-Learning ItÃ©ratif (70%)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Q-Learning Ã‰pisodique (55%)
â–ˆ Random Agent (5%)
```

### Vitesse d'Apprentissage

```
Ã‰pisodes pour atteindre 50% de succÃ¨s:
â€¢ Q-Learning ItÃ©ratif:  ~100 Ã©pisodes âš¡
â€¢ Q-Learning Ã‰pisodique: ~200 Ã©pisodes ğŸ¢
â€¢ Random Agent:         Jamais âŒ
```

---

## ğŸ” DiffÃ©rences ClÃ©s

### Value Iteration vs Q-Learning

| Aspect | Value Iteration | Q-Learning |
|--------|----------------|------------|
| **Type** | Planning (model-based) | Learning (model-free) |
| **Connaissance** | ConnaÃ®t la dynamique | DÃ©couvre par expÃ©rience |
| **Convergence** | Garantie (thÃ©orique) | Pas garantie |
| **Goal** | Statique | Dynamique âœ¨ |
| **Performance** | Excellente (95%) | Bonne (70%) |

### Q-Learning ItÃ©ratif vs Ã‰pisodique

| Aspect | ItÃ©ratif | Ã‰pisodique |
|--------|----------|------------|
| **Updates** | AprÃ¨s chaque step | Fin d'Ã©pisode |
| **Vitesse** | Plus rapide âš¡ | Plus lent ğŸ¢ |
| **StabilitÃ©** | Meilleure | Moins stable |
| **SuccÃ¨s** | 70% | 55% |
| **Utilisation mÃ©moire** | Faible | Buffer temporaire |

---

## ğŸ“ Concepts ImplÃ©mentÃ©s

### Algorithmes
- âœ… **Bellman Equation** (Value Iteration)
- âœ… **Q-Learning** (Temporal Difference)
- âœ… **Epsilon-Greedy** (Exploration/Exploitation)
- âœ… **Decay Scheduling** (Epsilon, Learning Rate)

### Techniques
- âœ… **Experience Replay** (Ã‰pisodique)
- âœ… **Online Learning** (ItÃ©ratif)
- âœ… **State Augmentation** (Position + Distance)
- âœ… **Reward Shaping** (Step cost, Goal reward)

### Visualisation
- âœ… **Heatmaps** (Values/Q-values)
- âœ… **Learning Curves** (Rewards, Success rate)
- âœ… **Real-time Updates** (Animation fluide)
- âœ… **Multi-panel Layout** (4 vues simultanÃ©es)

---

## ğŸ“ˆ MÃ©triques de Comparaison

L'outil `compare_methods.py` analyse **15+ mÃ©triques** :

### ğŸ“Š Performance
- RÃ©compense moyenne/max/min
- Longueur d'Ã©pisode moyenne
- Taux de succÃ¨s (100 derniers)

### âš¡ EfficacitÃ©
- Temps de convergence
- Nombre d'updates Q-table
- Ratio succÃ¨s/Ã©pisodes

### ğŸ¯ StabilitÃ©
- Variance des rÃ©compenses
- Ã‰cart-type longueurs
- CohÃ©rence des performances

### ğŸ”¬ Apprentissage
- Vitesse de convergence
- Exploration finale (epsilon)
- Taille Q-table

---

## ğŸ’¡ Enseignements

### 1. Planning vs Learning

**Value Iteration** (planning) est supÃ©rieur **SI** :
- âœ… On connaÃ®t la dynamique de l'environnement
- âœ… Le goal est statique
- âœ… On peut calculer toutes les transitions

**Q-Learning** (learning) est nÃ©cessaire **SI** :
- âœ… Environnement inconnu
- âœ… Goal dynamique
- âœ… Trop d'Ã©tats pour calculer exhaustivement

### 2. ItÃ©ratif vs Ã‰pisodique

**Updates immÃ©diates** (itÃ©ratif) battent **updates diffÃ©rÃ©es** (Ã©pisodique) :
- âš¡ Apprentissage plus rapide (+30% succÃ¨s)
- ğŸ“ˆ Convergence plus stable
- ğŸ¯ Meilleure utilisation des transitions

### 3. Importance de la Baseline

L'agent alÃ©atoire (5% succÃ¨s) prouve que :
- ğŸ§  L'apprentissage apporte une **vraie valeur** (+65% vs random)
- ğŸ¯ Le problÃ¨me n'est **pas trivial**
- ğŸ“Š Les gains sont **mesurables et significatifs**

---

## ğŸ› ï¸ Configuration

Fichiers de configuration disponibles :

```json
// Value Iteration/config.json
{
  "grid_size": 5,
  "start_pos": [0, 0],
  "goal_pos": [4, 4],
  "obstacles": [[2, 2]],
  "animation_speed": 0.5,
  "save_figures": true
}
```

**ParamÃ¨tres modifiables** :
- Taille de grille (3Ã—3 Ã  10Ã—10)
- Positions start/goal
- Liste d'obstacles
- Vitesse d'animation
- HyperparamÃ¨tres RL (epsilon, alpha, gamma)

---

## ğŸ“š Ressources

### Documentation Interne
- [`Value Iteration/README.md`](Value%20Iteration/README.md) - Planning classique
- [`Q-learning/README.md`](Q-learning/README.md) - Comparaison des mÃ©thodes
- [`Value Iteration Random/README.md`](Value%20Iteration%20Random/README.md) - Baseline

### Concepts RL
- **Bellman Equation** : Ã‰quation de rÃ©currence pour valeurs optimales
- **Q-Learning** : Apprentissage off-policy par diffÃ©rence temporelle
- **Epsilon-Greedy** : Balance exploration (random) / exploitation (greedy)
- **Decay** : RÃ©duction progressive de l'exploration

---

## ğŸ¯ Cas d'Usage

### PÃ©dagogique
- ğŸ“– Comprendre les bases du RL
- ğŸ”¬ Comparer planning vs learning
- ğŸ“Š Visualiser l'apprentissage
- ğŸ“ ExpÃ©rimenter avec les hyperparamÃ¨tres

### Recherche
- ğŸ§ª Baseline pour nouveaux algorithmes
- ğŸ“ˆ Benchmark sur GridWorld
- ğŸ” Analyse comparative de mÃ©thodes
- ğŸ“Š GÃ©nÃ©ration de mÃ©triques

### DÃ©veloppement
- ğŸ—ï¸ Architecture modulaire rÃ©utilisable
- ğŸ”§ Interface Gymnasium-style
- ğŸ“¦ Code propre et documentÃ©
- âœ… Facile Ã  Ã©tendre

---

## ğŸš€ Extensions Possibles

### AmÃ©liorations RL
- [ ] Double Q-Learning (rÃ©duire surestimation)
- [ ] Prioritized Experience Replay
- [ ] SARSA (on-policy)
- [ ] n-step TD methods

### Environnement
- [ ] Grilles plus grandes (10Ã—10, 20Ã—20)
- [ ] Obstacles mobiles
- [ ] Multiples goals
- [ ] RÃ©compenses intermÃ©diaires

### Visualisation
- [ ] Trajectoires colorÃ©es
- [ ] Graphiques 3D des Q-values
- [ ] Animation exportable (GIF/MP4)
- [ ] Dashboard interactif

---

## ğŸ“Š Exemple de Sortie

```
==========================================================
Q-LEARNING ITÃ‰RATIF - DYNAMIC GOAL
==========================================================
Nombre d'Ã©pisodes: 500
Taille de la grille: 5x5

Ã‰pisode 500/500
  RÃ©compense moyenne (10 derniers): 7.84
  Longueur moyenne (10 derniers): 38.2
  Taux de succÃ¨s (100 derniers): 70.0%
  Epsilon actuel: 0.05
  Q-table size: 143 entrÃ©es
  Updates effectuÃ©s: 19234

==========================================================
ENTRAÃNEMENT TERMINÃ‰
==========================================================

Statistiques finales:
  RÃ©compense moyenne (100 derniers): 6.29
  Longueur moyenne (100 derniers): 45.13
  Taux de succÃ¨s (100 derniers): 70.0%
  Q-table finale: 143 state-action pairs

âœ“ Courbes sauvegardÃ©es dans results_iterative/
âœ“ Statistiques sauvegardÃ©es dans results_iterative/
```

---

## ğŸ¤ Contribution

Ce projet est Ã©ducatif et ouvert aux amÃ©liorations :
- ğŸ› Signaler des bugs
- ğŸ’¡ Proposer des features
- ğŸ“– AmÃ©liorer la documentation
- ğŸ¨ Optimiser les visualisations

---

## ğŸ“œ Licence

Projet Ã©ducatif - Utilisation libre pour apprentissage et recherche.

---

## ğŸ‘¨â€ğŸ’» Auteur

Projet de Reinforcement Learning classique - DÃ©cembre 2025

**Technologies** : Python 3.8+, NumPy, Matplotlib  
**Frameworks** : Aucun (implÃ©mentation from scratch)  
**Inspiration** : Sutton & Barto - "Reinforcement Learning: An Introduction"

---

<div align="center">

**â­ N'oubliez pas de comparer les 4 mÃ©thodes pour voir la puissance de l'apprentissage ! â­**

[Value Iteration](#-value-iteration-planning) â€¢ [Q-Learning ItÃ©ratif](#-q-learning-itÃ©ratif-meilleure-performance) â€¢ [Q-Learning Ã‰pisodique](#-q-learning-Ã©pisodique) â€¢ [Random Baseline](#-random-agent-baseline)

</div>
